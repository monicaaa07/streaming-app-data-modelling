## Date created  
2/9/2021  

## Project Title  
Sparkify Music Streaming Data Modelling  

## Description  

#### Problem Statement:  
    
A music streaming app by Sparkify have been collecting data on songs and user activity. They are interested in understanding what songs users are listening to.The data resides in JSON logs and there is no way to query it. They will need a database schema and ETL pipeline to query this data and access it for analytics.  


#### Inputs: 
    
The data is present in "data" folder in the following 2 JSON files:  

    Song files: This is a subset from the Million Song Dataset. Each file contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.  
    song_data/A/B/C/TRABCEI128F424C983.json  
    song_data/A/A/B/TRAABJL12903CDCF1A.json  

    And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.  
 
     {"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}  
 
    Log files are generated by this event simulator based on the songs in the dataset above. They are partitioned by year and month. For example, here are filepaths to two files in this dataset.  

    log_data/2018/11/2018-11-12-events.json
    log_data/2018/11/2018-11-13-events.json
    And below is an example of what the data in a log file looks like.  

      {"artist":null,"auth":"LoggedIn","firstName":"Walter","gender":"M","itemInSession":0,"lastName":"Frye","length":null,"level":"free","location":"San Francisco-Oakland-Hayward, CA","method":"GET","page":"Home","registration":1540919166796.0,"sessionId":38,"song":null,"status":200,"ts":1541105830796,"userAgent":"\"Mozilla\/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/36.0.1985.143 Safari\/537.36\"","userId":"39"}  

#### Database Schema Design Considerations:    

From the data in both JSON files, the "objects" are  songs, artists, users. These form the dimension. Addition of a time dimension because of the presence of timestamp in the logs, help in querying or indexing by time.  
The dimension tables with their attributes as understood from the JSON files are as follows  
    songs - song_id, title, artist_id, year, duration  
    artists - artist_id, name, location, latitude, longitude  
    users - user_id, first_name, last_name, gender, level  
    time - start_time, hour, day, week, month, year, weekday  
  
The fact table has to be denormalized and optimzed for ease of querying according to the purpose. The level of the fact table must be at user level along with the songs they have listened to. The fact tables needs to have keys of all the dimension tables,i.e; song_id,artist_id,user_id & start_time, The fact table, songplays has the following columns - songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent  

#### ETL Schema Design Considerations:  

The source data is present in "data" folder. "Inputs" section above details the structure of the Song files & Log files.  

In **etl.py**, data for the fields of songs & artists dimension tables is read from Song files.User dim tables & details are got from Log files. Time dimension is populated based on every timestamp in the Log Files. **After all the dimensions are loaded the fact table is populated**. The user level info is present in the log files however the artist_id & song_id is in the artists & songs dim tables. They are matched based on song title, artist name & song duration and the song ids and artist ids are obtained in relation with the user who heard that particular song.  

The queries to get song_id & artist_id as well as create, drop & insert tables are in **sql_queries.py** for speration of concerns, good design practice and easier editing to queries in case of changes in requirement.  

**create_tables.py** drops tables if they exists & creates them while insert is done on etl.py. the queries for this is got by importing sql_queries in these scripts.

#### Execution Steps:  
Postgres must be installed on the system, The database must be created
Execute create_tables.py. If error occures, comment drop and create the tables. Run etl.py. Run test.py to check if tables were loaded correctly. Restart kernal after test.py to close databse connection.  

##### Credits
Million Song Dataset : http://millionsongdataset.com/
Defining Postgres autoincrement : https://www.tutorialspoint.com/postgresql/postgresql_using_autoincrement.htm

##### Need to be added to Readme.md:
[Optional] Provide example queries and results for song play analysis.
Diagramtic representation of Database Schema & ETL